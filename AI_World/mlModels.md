---
markmap:
  colorFreezeLevel: 2
  initialExpandLevel: 1
---

# ğŸŒ Artificial Intelligence (AI)

## 1. Major Branches

### ğŸ¤– Machine Learning (ML)
- Algorithms that learn patterns from data
- Two core families:
  - **Classical ML (Non-Neural)**
  - **Deep Learning (Neural Networkâ€“Based)**

## 2. ğŸ§© Classical / Non-Neural Machine Learning
### ğŸ§® Core Idea
- Each model is a **distinct mathematical module**
- Learns via **explicit formulas**, **probabilistic methods**, or **optimization**
- No concept of neurons or layers

### ğŸ“Š Common Models
- **Linear Regression**
  - Predicts continuous values
  - Minimizes Mean Squared Error
- **Logistic Regression**
  - Classifies with probability (sigmoid)
  - Uses maximum likelihood estimation
- **SVM (Support Vector Machine)**
  - Finds max-margin hyperplane
  - Uses convex optimization
- **Decision Tree**
  - Splits data using impurity (Gini/entropy)
  - Produces ifâ€“else rules
- **Random Forest**
  - Ensemble of many trees
  - Averaging/voting improves accuracy
- **KNN (K-Nearest Neighbors)**
  - Memorizes data
  - Predicts based on nearby samples
- **Naive Bayes**
  - Uses Bayesâ€™ theorem with independence assumption
- **PCA (Principal Component Analysis)**
  - Finds directions of maximum variance
  - Uses linear algebra (eigenvectors)

### âš™ï¸ Learning Mechanism
- Optimizes **explicit objective functions**
- Examples:
  - Linear Regression â†’ minimize squared error
  - SVM â†’ maximize class margin
  - Decision Tree â†’ minimize impurity
- Uses **analytical or geometric optimization**, not backpropagation

## 3. ğŸ§  Neural Network / Deep Learning
### ğŸ’¡ Foundational Unit: The Perceptron
- Mathematical formula: `y = f(Î£ wáµ¢xáµ¢ + b)`
- `f()` = activation (ReLU, sigmoid, tanh, etc.)
- Learns through **weight updates**

### ğŸ”„ Learning Mechanism
- **Gradient Descent + Backpropagation**
  - Forward pass â†’ compute output
  - Compare to true output (loss)
  - Backward pass â†’ update weights via gradients
- Objective: minimize prediction error

### ğŸ§± Building Structure
- **Layers**
  - Input â†’ Hidden â†’ Output
- **Networks**
  - Stack multiple layers â†’ feedforward networks
- **Activation Functions**
  - Add nonlinearity â†’ allow complex mapping

### ğŸ—ï¸ Major Neural Architectures
- **Feedforward NN (FNN / MLP)**
  - Simple multilayer network
- **CNN (Convolutional Neural Network)**
  - Local connections, weight sharing
  - Best for images
- **RNN (Recurrent Neural Network)**
  - Temporal memory
  - Best for sequences
- **LSTM / GRU**
  - Improved RNNs for long-term memory
- **Transformer**
  - Uses self-attention instead of recurrence
  - Foundation for GPT, BERT, etc.
- **Autoencoder**
  - Learns compressed representations
- **GAN (Generative Adversarial Network)**
  - Generator + Discriminator
- **Diffusion Models**
  - Denoising-based generation (DALLÂ·E, Stable Diffusion)
- **GNN (Graph Neural Network)**
  - Works on graph-structured data

## 4. ğŸ§¬ Scaling Up â†’ Foundation Models
- **LLMs (Large Language Models)**
  - Massive Transformer networks (GPT, Claude, Gemini)
- **Vision Foundation Models**
  - CNN/ViT-based (SAM, CLIP, DINOv2)
- **Multimodal Models**
  - Combine text, image, audio (GPT-4, Gemini 1.5)

## 5. ğŸ§° AI Systems and Applications
- Chatbots (ChatGPT, Claude)
- Image generators (DALLÂ·E, Midjourney)
- Speech systems (Whisper)
- Recommendation engines
- Autonomous agents (AutoGPT, Devin)
- Robotics and control systems

## 6. âš–ï¸ Neural vs. Non-Neural Summary
| Feature | Neural Network Models | Classical ML Models |
|----------|----------------------|---------------------|
| **Base Unit** | Perceptron (universal) | Distinct mathematical model |
| **Learning** | Gradient descent, backprop | Explicit optimization or logic |
| **Data Need** | High | Lowâ€“medium |
| **Interpretability** | Low (black box) | High (transparent math) |
| **Computation** | Heavy (GPU) | Light (CPU) |
| **Power** | High for complex patterns | Limited for simple ones |

## 7. ğŸ§­ Big Picture Hierarchy
